{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from autograd.tensor import Tensor as tn\n",
    "from autograd.dropout import *\n",
    "from autograd.parameter import Parameter as pr\n",
    "from autograd.flatten import Flatten as fl\n",
    "from autograd.module import Module\n",
    "from autograd.nn import Sequential\n",
    "from autograd.linear import Linear\n",
    "from autograd.act import Relu, Sigmoid \n",
    "from autograd.loss import CEL\n",
    "from autograd.optim import SGD\n",
    "from autograd.grad_check import Grad_Check as gc\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from types import SimpleNamespace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = {'W1': np.array([[-0.3224172 , -0.38405435,  1.13376944, -1.09989127],\n",
    "       [-0.17242821, -0.87785842,  0.04221375,  0.58281521],\n",
    "       [-1.10061918,  1.14472371,  0.90159072,  0.50249434],\n",
    "       [ 0.90085595, -0.68372786, -0.12289023, -0.93576943],\n",
    "       [-0.26788808,  0.53035547, -0.69166075, -0.39675353]]), \n",
    "     'b1': np.array([[-0.6871727 ],\n",
    "       [-0.84520564],\n",
    "       [-0.67124613],\n",
    "       [-0.0126646 ],\n",
    "       [-1.11731035]]), \n",
    "     'W2': np.array([[ 0.2344157 ,  1.65980218,  0.74204416, -0.19183555, -0.88762896],\n",
    "       [-0.74715829,  1.6924546 ,  0.05080775, -0.63699565,  0.19091548],\n",
    "       [ 2.10025514,  0.12015895,  0.61720311,  0.30017032, -0.35224985]]), \n",
    "     'b2': np.array([[-1.1425182 ],\n",
    "       [-0.34934272],\n",
    "       [-0.20889423]]), \n",
    "     'W3': np.array([[0.58662319, 0.83898341, 0.93110208]]), \n",
    "     'b3': np.array([[0.28558733]])\n",
    "    }\n",
    "\n",
    "X = np.asarray([[ 1.62434536, -0.61175641, -0.52817175],\n",
    " [-1.07296862,  0.86540763, -2.3015387 ],\n",
    " [ 1.74481176, -0.7612069,   0.3190391 ],\n",
    " [-0.24937038,  1.46210794, -2.06014071]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_x = tn(X)\n",
    "train_y = tn(np.asarray([1, 1, 0]))\n",
    "Y = np.asarray([1, 1, 0])\n",
    "W1 = d['W1']\n",
    "b1 = d['b1'] \n",
    "W2 = d['W2'] \n",
    "b2 = d['b2']\n",
    "W3 = d['W3']\n",
    "b3 = d['b3']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "Linear.lambda_ = None\n",
    "Linear.lambda_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "mod2 = Sequential([\n",
    "         Linear(data_in_w=W1,data_in_b=b1),\n",
    "            Relu(),\n",
    "         Linear(data_in_w=W2,data_in_b=b2),\n",
    "            Relu(),\n",
    "        Linear(data_in_w=W3,data_in_b=b3),\n",
    "            Sigmoid()])\n",
    "\n",
    "#train_x = tn(np.random.randn(20,2))\n",
    "#train_y = tn(np.asarray([[1,0]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(2.4078333901177116, requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "optimizer = SGD(lr=.075)\n",
    "\n",
    "\n",
    "out = mod2.forward(train_x)\n",
    "cout = CEL(out,train_y)\n",
    "loss = cout.sum()\n",
    "\n",
    "loss.backward()\n",
    "\n",
    "optimizer.save_params(mod2)\n",
    "#optimizer.step(mod2)\n",
    "\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "47"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "parameters_values, _ = gc().dictionary_to_vector()\n",
    "grad = gc().gradients_to_vector()\n",
    "\n",
    "num_parameters = parameters_values.shape[0]\n",
    "J_plus = np.zeros((num_parameters, 1))\n",
    "J_minus = np.zeros((num_parameters, 1))\n",
    "gradapprox = np.zeros((num_parameters, 1))\n",
    "\n",
    "epsilon = 1e-7\n",
    "num_parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Module.param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.778643084091743e-07 6.770359682745232\n",
      "\u001b[92mYour backward propagation works perfectly fine! difference = 7.058181999208215e-08\u001b[0m\n",
      "7.058181999208215e-08\n"
     ]
    }
   ],
   "source": [
    "for i in range(num_parameters):\n",
    "    \n",
    "    thetaplus =  np.copy(parameters_values)                                      \n",
    "    thetaplus[i][0] = thetaplus[i][0]+epsilon \n",
    "    dic = gc().vector_to_dictionary(thetaplus)\n",
    "    \n",
    "\n",
    "    \n",
    "    #J_plus[i],_ = forward_propagation_n(train_x.data,train_y.data,dic)\n",
    "    mod3 = Sequential([])\n",
    "    mod3.add(Linear(data_in_w= dic['w1'],data_in_b=dic['b1']))\n",
    "    mod3.add(Relu())\n",
    "    mod3.add(Linear(data_in_w= dic['w3'],data_in_b=dic['b3']))\n",
    "    mod3.add(Relu())\n",
    "    mod3.add(Linear(data_in_w= dic['w5'],data_in_b=dic['b5']))\n",
    "    mod3.add(Sigmoid())\n",
    "    out3 = mod3.forward(train_x)\n",
    "    cout3 = CEL(out3,train_y,mod3)\n",
    "    \n",
    "    J_plus[i] = cout3.sum().data\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    thetaminus = np.copy(parameters_values)                                       \n",
    "    thetaminus[i][0] = thetaminus[i][0] - epsilon \n",
    "    dic = gc().vector_to_dictionary(thetaminus)\n",
    "    #J_minus[i],_ = forward_propagation_n(train_x.data,train_y.data,dic)\n",
    "    \n",
    "    \n",
    "    mod4 = Sequential([])\n",
    "    mod4.add(Linear(data_in_w= dic['w1'],data_in_b=dic['b1']))\n",
    "    mod4.add(Relu())\n",
    "    mod4.add(Linear(data_in_w= dic['w3'],data_in_b=dic['b3']))\n",
    "    mod4.add(Relu())\n",
    "    mod4.add(Linear(data_in_w= dic['w5'],data_in_b=dic['b5']))\n",
    "    mod4.add(Sigmoid())\n",
    "    \n",
    "    \n",
    "    out4 = mod4.forward(train_x)\n",
    "    cout4 = CEL(out4,train_y,mod4)\n",
    "    J_minus[i] = cout4.sum().data\n",
    "    \n",
    "\n",
    "    gradapprox[i] = (J_plus[i] - J_minus[i])/ (2*epsilon)\n",
    "\n",
    "    \n",
    "\n",
    "numerator = np.linalg.norm(gradapprox-grad)                                     \n",
    "denominator = np.linalg.norm(grad) + np.linalg.norm(gradapprox)                   \n",
    "difference = numerator / denominator                                       \n",
    "\n",
    "print(numerator,denominator)\n",
    "\n",
    "if difference > 1e-7:\n",
    "    print (\"\\033[93m\" + \"There is a mistake in the backward propagation! difference = \" + str(difference) + \"\\033[0m\")\n",
    "else:\n",
    "    print (\"\\033[92m\" + \"Your backward propagation works perfectly fine! difference = \" + str(difference) + \"\\033[0m\")\n",
    "\n",
    "\n",
    "print(difference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
