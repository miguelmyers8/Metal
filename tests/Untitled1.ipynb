{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from autograd.tensor import Tensor as tn\n",
    "from autograd.dropout import *\n",
    "from autograd.parameter import Parameter as pr\n",
    "from autograd.flatten import Flatten as fl\n",
    "from autograd.module import Module\n",
    "from autograd.nn import Sequential\n",
    "from autograd.linear import Linear\n",
    "from autograd.act import Relu, Sigmoid \n",
    "from autograd.loss import CEL\n",
    "from autograd.optim import GD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(2.973981747096818, requires_grad=True)\n",
      "Tensor(1.1525848858146912, requires_grad=True)\n",
      "Tensor(0.8092322200712426, requires_grad=True)\n",
      "Tensor(0.648900955935711, requires_grad=True)\n",
      "Tensor(0.5302958839700223, requires_grad=True)\n",
      "Tensor(0.42924981305548227, requires_grad=True)\n",
      "Tensor(0.3420929727522987, requires_grad=True)\n",
      "Tensor(0.26729885677431675, requires_grad=True)\n",
      "Tensor(0.2040379153116074, requires_grad=True)\n",
      "Tensor(0.15171079130161946, requires_grad=True)\n",
      "0.010184049606323242\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import numpy\n",
    "np.random.seed(0)\n",
    "\n",
    "data = tn(np.array([[0,0],[0,1],[1,0],[1,1]]),True)\n",
    "target = tn(np.array([[0],[1],[0],[1]]),True)\n",
    "\n",
    "w = list()\n",
    "weights_0_1 = tn(np.random.rand(2,3),True)\n",
    "weights_1_2 = tn(np.random.rand(3,1),True)\n",
    "w.append(weights_0_1)\n",
    "w.append(weights_1_2)\n",
    "\n",
    "\n",
    "\n",
    "start = time.time()\n",
    "for i in range(10):\n",
    "    \n",
    "    # Predict\n",
    "    layer_1 = data @ weights_0_1\n",
    "    layer_2 = layer_1 @ weights_1_2\n",
    "    # Compare\n",
    "    diff = (layer_2 - target)\n",
    "    sqdiff = (diff * diff)\n",
    "    loss = sqdiff.sum() # mean squared error loss\n",
    "\n",
    "    # Learn: this is the backpropagation piece\n",
    "    loss.backward()\n",
    "\n",
    "    for w_ in w:\n",
    "        w_.data -= w_.grad.data * 0.1\n",
    "        w_.zero_grad()\n",
    "    print(loss)\n",
    "end = time.time()\n",
    "print(end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ True  True  True  True  True]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "class Tensor (object):\n",
    "    \n",
    "    def __init__(self,data,\n",
    "                 autograd=False,\n",
    "                 creators=None,\n",
    "                 creation_op=None,\n",
    "                 id=None):\n",
    "        \n",
    "        self.data = np.array(data)\n",
    "        self.autograd = autograd\n",
    "        self.grad = None\n",
    "        if(id is None):\n",
    "            self.id = np.random.randint(0,100000)\n",
    "        else:\n",
    "            self.id = id\n",
    "        \n",
    "        self.creators = creators\n",
    "        self.creation_op = creation_op\n",
    "        self.children = {}\n",
    "        \n",
    "        if(creators is not None):\n",
    "            for c in creators:\n",
    "                if(self.id not in c.children):\n",
    "                    c.children[self.id] = 1\n",
    "                else:\n",
    "                    c.children[self.id] += 1\n",
    "\n",
    "    def all_children_grads_accounted_for(self):\n",
    "        for id,cnt in self.children.items():\n",
    "            if(cnt != 0):\n",
    "                return False\n",
    "        return True \n",
    "        \n",
    "    def backward(self,grad=None, grad_origin=None):\n",
    "        if(self.autograd):\n",
    " \n",
    "            if(grad is None):\n",
    "                grad = Tensor(np.ones_like(self.data))\n",
    "\n",
    "            if(grad_origin is not None):\n",
    "                if(self.children[grad_origin.id] == 0):\n",
    "                    raise Exception(\"cannot backprop more than once\")\n",
    "                else:\n",
    "                    self.children[grad_origin.id] -= 1\n",
    "\n",
    "            if(self.grad is None):\n",
    "                self.grad = grad\n",
    "            else:\n",
    "                self.grad += grad\n",
    "            \n",
    "            # grads must not have grads of their own\n",
    "            assert grad.autograd == False\n",
    "            \n",
    "            # only continue backpropping if there's something to\n",
    "            # backprop into and if all gradients (from children)\n",
    "            # are accounted for override waiting for children if\n",
    "            # \"backprop\" was called on this variable directly\n",
    "            if(self.creators is not None and \n",
    "               (self.all_children_grads_accounted_for() or \n",
    "                grad_origin is None)):\n",
    "\n",
    "                if(self.creation_op == \"add\"):\n",
    "                    self.creators[0].backward(self.grad, self)\n",
    "                    self.creators[1].backward(self.grad, self)\n",
    "                    \n",
    "                if(self.creation_op == \"sub\"):\n",
    "                    self.creators[0].backward(Tensor(self.grad.data), self)\n",
    "                    self.creators[1].backward(Tensor(self.grad.__neg__().data), self)\n",
    "\n",
    "                if(self.creation_op == \"mul\"):\n",
    "                    new = self.grad * self.creators[1]\n",
    "                    self.creators[0].backward(new , self)\n",
    "                    new = self.grad * self.creators[0]\n",
    "                    self.creators[1].backward(new, self)                    \n",
    "                    \n",
    "                if(self.creation_op == \"mm\"):\n",
    "                    c0 = self.creators[0]\n",
    "                    c1 = self.creators[1]\n",
    "                    new = self.grad.mm(c1.transpose())\n",
    "                    c0.backward(new)\n",
    "                    new = self.grad.transpose().mm(c0).transpose()\n",
    "                    c1.backward(new)\n",
    "                    \n",
    "                if(self.creation_op == \"transpose\"):\n",
    "                    self.creators[0].backward(self.grad.transpose())\n",
    "\n",
    "                if(\"sum\" in self.creation_op):\n",
    "                    dim = int(self.creation_op.split(\"_\")[1])\n",
    "                    self.creators[0].backward(self.grad.expand(dim,\n",
    "                                                               self.creators[0].data.shape[dim]))\n",
    "\n",
    "                if(\"expand\" in self.creation_op):\n",
    "                    dim = int(self.creation_op.split(\"_\")[1])\n",
    "                    self.creators[0].backward(self.grad.sum(dim))\n",
    "                    \n",
    "                if(self.creation_op == \"neg\"):\n",
    "                    self.creators[0].backward(self.grad.__neg__())\n",
    "                    \n",
    "    def __add__(self, other):\n",
    "        if(self.autograd and other.autograd):\n",
    "            return Tensor(self.data + other.data,\n",
    "                          autograd=True,\n",
    "                          creators=[self,other],\n",
    "                          creation_op=\"add\")\n",
    "        return Tensor(self.data + other.data)\n",
    "\n",
    "    def __neg__(self):\n",
    "        if(self.autograd):\n",
    "            return Tensor(self.data * -1,\n",
    "                          autograd=True,\n",
    "                          creators=[self],\n",
    "                          creation_op=\"neg\")\n",
    "        return Tensor(self.data * -1)\n",
    "    \n",
    "    def __sub__(self, other):\n",
    "        if(self.autograd and other.autograd):\n",
    "            return Tensor(self.data - other.data,\n",
    "                          autograd=True,\n",
    "                          creators=[self,other],\n",
    "                          creation_op=\"sub\")\n",
    "        return Tensor(self.data - other.data)\n",
    "    \n",
    "    def __mul__(self, other):\n",
    "        if(self.autograd and other.autograd):\n",
    "            return Tensor(self.data * other.data,\n",
    "                          autograd=True,\n",
    "                          creators=[self,other],\n",
    "                          creation_op=\"mul\")\n",
    "        return Tensor(self.data * other.data)    \n",
    "\n",
    "    def sum(self, dim):\n",
    "        if(self.autograd):\n",
    "            return Tensor(self.data.sum(dim),\n",
    "                          autograd=True,\n",
    "                          creators=[self],\n",
    "                          creation_op=\"sum_\"+str(dim))\n",
    "        return Tensor(self.data.sum(dim))\n",
    "    \n",
    "    def expand(self, dim,copies):\n",
    "\n",
    "        trans_cmd = list(range(0,len(self.data.shape)))\n",
    "        trans_cmd.insert(dim,len(self.data.shape))\n",
    "        new_data = self.data.repeat(copies).reshape(list(self.data.shape) + [copies]).transpose(trans_cmd)\n",
    "        \n",
    "        if(self.autograd):\n",
    "            return Tensor(new_data,\n",
    "                          autograd=True,\n",
    "                          creators=[self],\n",
    "                          creation_op=\"expand_\"+str(dim))\n",
    "        return Tensor(new_data)\n",
    "    \n",
    "    def transpose(self):\n",
    "        if(self.autograd):\n",
    "            return Tensor(self.data.transpose(),\n",
    "                          autograd=True,\n",
    "                          creators=[self],\n",
    "                          creation_op=\"transpose\")\n",
    "        \n",
    "        return Tensor(self.data.transpose())\n",
    "    \n",
    "    def mm(self, x):\n",
    "        if(self.autograd):\n",
    "            return Tensor(self.data.dot(x.data),\n",
    "                          autograd=True,\n",
    "                          creators=[self,x],\n",
    "                          creation_op=\"mm\")\n",
    "        return Tensor(self.data.dot(x.data))\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return str(self.data.__repr__())\n",
    "    \n",
    "    def __str__(self):\n",
    "        return str(self.data.__str__())  \n",
    "    \n",
    "a = Tensor([1,2,3,4,5], autograd=True)\n",
    "b = Tensor([2,2,2,2,2], autograd=True)\n",
    "c = Tensor([5,4,3,2,1], autograd=True)\n",
    "\n",
    "d = a + b\n",
    "e = b + c\n",
    "f = d + e\n",
    "\n",
    "f.backward(Tensor(np.array([1,1,1,1,1])))\n",
    "\n",
    "print(b.grad.data == np.array([2,2,2,2,2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2.97398178]\n",
      "[1.15258489]\n",
      "[0.80923222]\n",
      "[0.64890096]\n",
      "[0.53029588]\n",
      "[0.42924981]\n",
      "[0.34209297]\n",
      "[0.26729886]\n",
      "[0.20403792]\n",
      "[0.15171079]\n",
      "0.017199277877807617\n"
     ]
    }
   ],
   "source": [
    "import numpy\n",
    "np.random.seed(0)\n",
    "\n",
    "data = Tensor(np.array([[0,0],[0,1],[1,0],[1,1]]), autograd=True)\n",
    "target = Tensor(np.array([[0],[1],[0],[1]]), autograd=True)\n",
    "\n",
    "w = list()\n",
    "w.append(Tensor(np.array([[0.85794562 ,0.84725174 ,0.6235637 ],\n",
    " [0.38438171 ,0.29753461 ,0.05671298]]), autograd=True))\n",
    "w.append(Tensor(np.array([[0.47766512],\n",
    " [0.81216873],\n",
    " [0.47997717]]), autograd=True))\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "for i in range(10):\n",
    "\n",
    "    # Predict\n",
    "    pred = data.mm(w[0]).mm(w[1])\n",
    "    \n",
    "    # Compare\n",
    "    loss = ((pred - target)*(pred - target)).sum(0)\n",
    "    \n",
    "    # Learn\n",
    "    loss.backward(Tensor(np.ones_like(loss.data)))\n",
    "\n",
    "    for w_ in w:\n",
    "        w_.data -= w_.grad.data * 0.1\n",
    "        w_.grad.data *= 0\n",
    "\n",
    "    print(loss)\n",
    "end = time.time()\n",
    "print(end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.47766512]\n",
      " [0.81216873]\n",
      " [0.47997717]]\n",
      "2.9739817759207106\n",
      "1.1386077634035665\n",
      "0.9503752598761414\n",
      "0.8477935879208961\n",
      "0.7687994191412023\n",
      "0.7000083625355942\n",
      "0.6374749719001984\n",
      "0.579685958415488\n",
      "0.5258833043645329\n",
      "0.4756057918655521\n"
     ]
    }
   ],
   "source": [
    "import numpy\n",
    "np.random.seed(0)\n",
    "\n",
    "data = np.array([[0,0],[0,1],[1,0],[1,1]])\n",
    "target = np.array([[0],[1],[0],[1]])\n",
    "\n",
    "weights_0_1 = np.array([[0.85794562 ,0.84725174 ,0.6235637 ],\n",
    " [0.38438171 ,0.29753461 ,0.05671298]])\n",
    "\n",
    "weights_1_2 = np.array([[0.47766512],\n",
    " [0.81216873],\n",
    " [0.47997717]])\n",
    "print(weights_1_2)\n",
    "\n",
    "for i in range(10):\n",
    "    \n",
    "    # Predict\n",
    "    layer_1 = data @ weights_0_1\n",
    "    layer_2 = layer_1 @ weights_1_2\n",
    "\n",
    "    # Compare\n",
    "    diff = (layer_2 - target)\n",
    "    sqdiff = (diff * diff)\n",
    "    loss = sqdiff.sum(0) # mean squared error loss\n",
    "\n",
    "    # Learn: this is the backpropagation piece\n",
    "    layer_1_grad = diff.dot(weights_1_2.transpose())\n",
    "    weight_1_2_update = layer_1.transpose().dot(diff)\n",
    "    weight_0_1_update = data.transpose().dot(layer_1_grad)\n",
    "    \n",
    "    weights_1_2 -= weight_1_2_update * 0.1\n",
    "    weights_0_1 -= weight_0_1_update * 0.1\n",
    "    print(loss[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Tensor([[[[-0.94035354  0.92959433 -1.06279492]\n",
       "   [-0.88640627  1.92134696 -0.45978052]]\n",
       "\n",
       "  [[-1.08903444  0.98411729 -1.15920632]\n",
       "   [-0.43653709  1.00924453  0.71338957]]\n",
       "\n",
       "  [[-0.72805772  0.83951646  1.23902098]\n",
       "   [-1.78480389 -0.79618584 -1.40054127]]]\n",
       "\n",
       "\n",
       " [[[-0.18435058 -1.39119312  0.03625974]\n",
       "   [-0.81440556  0.69737282 -1.73742924]]\n",
       "\n",
       "  [[ 0.1158557   0.36565145 -0.07392347]\n",
       "   [-0.49351757  3.10153058  0.85875415]]\n",
       "\n",
       "  [[-1.15477553  0.94183434 -0.28213514]\n",
       "   [-0.97565467  0.09818669  0.90548995]]]\n",
       "\n",
       "\n",
       " [[[ 1.01874144 -0.11489885  1.74303872]\n",
       "   [-0.32187919  0.82957109 -0.207318  ]]\n",
       "\n",
       "  [[ 1.11799861  1.06424968  1.15132983]\n",
       "   [-0.77245771 -1.29363428  0.67702681]]\n",
       "\n",
       "  [[ 0.4240552  -0.48567617 -0.05169724]\n",
       "   [ 0.56705637  1.06783358  0.27159574]]]\n",
       "\n",
       "\n",
       " [[[ 0.61930177 -0.058626    1.2565714 ]\n",
       "   [ 0.2967472   0.39858573 -1.05317438]]\n",
       "\n",
       "  [[-0.63947627 -0.14852688 -1.57458215]\n",
       "   [-0.4956882  -0.11757479  0.37312016]]\n",
       "\n",
       "  [[-0.38602877 -1.23059312 -1.95827161]\n",
       "   [-0.51796269  0.5797988  -1.35373284]]]], requires_grad=False)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r = tn(np.random.randn(4,3,2,3))\n",
    "r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Tensor([[[-1.06279492 -0.45978052]\n",
       "  [-1.15920632  0.71338957]\n",
       "  [ 1.23902098 -1.40054127]]\n",
       "\n",
       " [[ 0.03625974 -1.73742924]\n",
       "  [-0.07392347  0.85875415]\n",
       "  [-0.28213514  0.90548995]]\n",
       "\n",
       " [[ 1.74303872 -0.207318  ]\n",
       "  [ 1.15132983  0.67702681]\n",
       "  [-0.05169724  0.27159574]]\n",
       "\n",
       " [[ 1.2565714  -1.05317438]\n",
       "  [-1.57458215  0.37312016]\n",
       "  [-1.95827161 -1.35373284]]], requires_grad=False)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r[...,2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "32\n",
      "64\n",
      "96\n"
     ]
    }
   ],
   "source": [
    "for i in range(0,100,32):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iterate_minibatches(inputs, targets, batchsize, shuffle=False):\n",
    "    assert inputs.shape[0] == targets.shape[0]\n",
    "    if shuffle:\n",
    "        indices = np.arange(inputs.shape[0])\n",
    "        np.random.shuffle(indices)\n",
    "    for start_idx in range(0, inputs.shape[0] - batchsize + 1, batchsize):\n",
    "        if shuffle:\n",
    "            excerpt = indices[start_idx:start_idx + batchsize]\n",
    "        else:\n",
    "            excerpt = slice(start_idx, start_idx + batchsize)\n",
    "            \n",
    "        inputs_ = inputs[excerpt]\n",
    "        inputs_ = inputs_.reshape(inputs_.shape[0], -1).T\n",
    "        \n",
    "        targets_ = targets[excerpt]\n",
    "        targets_ = targets_.reshape(targets_.shape[0], -1).T\n",
    "        \n",
    "        yield tn(inputs_), tn(targets_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Dataset1 import catnoncat\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = catnoncat.train_x() \n",
    "test_x = catnoncat.test_img() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#train_x = fl().forward(train_x)\n",
    "#test_x = fl().forward(test_x)\n",
    "#train_x = train_x.data\n",
    "#test_x = test_x.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_y = tn(catnoncat.train_y())\n",
    "test_y = tn(catnoncat.test_y())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = train_x.reshape(train_x.shape[0], -1).T\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_y = train_y.data.reshape(train_y.data.shape[0], -1).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(209, 1) (209, 12288)\n"
     ]
    }
   ],
   "source": [
    "X = train_x\n",
    "Y = train_y\n",
    "print(Y.shape,X.shape)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "mod2 = Sequential([\n",
    "         Linear(50,12288),\n",
    "            Relu(),\n",
    "        #Dropout(.9),\n",
    "         Linear(20,50),\n",
    "            Relu(),\n",
    "        #Dropout(.9),\n",
    "         Linear(10,20),\n",
    "            Relu(),\n",
    "        #Dropout(.9),\n",
    "         Linear(1,10),\n",
    "            Sigmoid()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 2.221715436379601\n",
      "100 1.8518230298011162\n",
      "200 1.7914018002493004\n",
      "300 1.689644361440539\n",
      "400 1.6266123291652475\n",
      "500 1.5191921787454636\n",
      "600 1.4633743939812938\n",
      "700 1.3467065202549793\n",
      "800 1.2843724277222521\n",
      "900 1.2068832745905715\n",
      "1000 1.1403792942773072\n",
      "1100 1.1031763508718893\n",
      "1200 1.0301783842499028\n",
      "1300 0.9906973379518542\n",
      "1400 0.9599488195978627\n",
      "1500 0.896955098834117\n",
      "1600 0.8431044032803237\n",
      "1700 0.8223686728629511\n",
      "1800 0.7740837210545408\n",
      "1900 0.7373953383519072\n",
      "2000 0.7121157358972231\n",
      "2100 0.6770496733791194\n",
      "2200 0.645676218853944\n",
      "2300 0.6128527772573465\n",
      "2400 0.5950146100774534\n",
      "2500 0.5721997715068396\n",
      "2600 0.537336608039728\n",
      "2700 0.5105893586469508\n",
      "2800 0.502366427829644\n",
      "2900 0.48792525762410655\n",
      "3000 0.4742774471603093\n",
      "3100 0.4633819990459521\n",
      "3200 0.4264421326516883\n",
      "3300 0.4151781195951647\n",
      "3400 0.3930785102576523\n",
      "3500 0.3993535531297261\n",
      "3600 0.38049525055702216\n",
      "3700 0.3571372202209674\n",
      "3800 0.3550808084136797\n",
      "3900 0.34814293900740095\n",
      "4000 0.32174032621091586\n",
      "4100 0.3212709732064312\n",
      "4200 0.3091148577050118\n",
      "4300 0.29956792156818357\n",
      "4400 0.2894025623426355\n",
      "4500 0.27624618879743923\n",
      "4600 0.2759398877497795\n",
      "4700 0.2550425581103539\n",
      "4800 0.251463476485587\n",
      "4900 0.24874806406219263\n",
      "5000 0.23664347109558168\n",
      "5100 0.23440408697597098\n",
      "5200 0.22441588292132492\n",
      "5300 0.2074740822707935\n",
      "5400 0.20813365005232826\n",
      "5500 0.20248740417827632\n",
      "5600 0.19888900452783842\n",
      "5700 0.18519895105938944\n",
      "5800 0.17500848094301347\n",
      "5900 0.18466658862830868\n",
      "6000 0.17499921360031134\n",
      "6100 0.16930462940264135\n",
      "6200 0.16908397160135746\n",
      "6300 0.1661890151328404\n",
      "6400 0.1585435346031115\n",
      "6500 0.15272449829919255\n",
      "6600 0.1503426650535086\n",
      "6700 0.14883249138086946\n",
      "6800 0.14204769486309662\n",
      "6900 0.13873673294883962\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "optimizer = GD(lr=.0075)\n",
    "\n",
    "\n",
    "costs = []\n",
    "\n",
    "\n",
    "for epoch in range(7000):\n",
    "\n",
    "    epoch_loss = 0.0\n",
    "    \n",
    "    for batch in iterate_minibatches(X, Y, 64, shuffle=True):\n",
    "        x_batch, y_batch = batch\n",
    "        train_x, train_y = x_batch, y_batch\n",
    "        for l in mod2.layers:\n",
    "            l.zero_grad()\n",
    "\n",
    "        out = mod2.forward(train_x)\n",
    "        cout = CEL(out,train_y)\n",
    "        loss = cout.sum()\n",
    "\n",
    "        loss.backward()\n",
    "        epoch_loss += loss.data\n",
    "\n",
    "        for l in mod2.layers:\n",
    "            optimizer.step(l)\n",
    "\n",
    "\n",
    "    if epoch % 100 == 0:\n",
    "        print(epoch, epoch_loss)\n",
    "    if epoch % 100 == 0:\n",
    "        costs.append(epoch_loss)\n",
    "\n",
    "\n",
    "# plot the cost\n",
    "plt.plot(np.squeeze(costs))\n",
    "plt.ylabel('cost')\n",
    "plt.xlabel('iterations (per tens)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data shape: (12288, 50)\n"
     ]
    }
   ],
   "source": [
    "test_x = catnoncat.test_img() \n",
    "test_x = fl().forward(test_x)\n",
    "test_y = tn(catnoncat.test_y())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.76\n"
     ]
    }
   ],
   "source": [
    "mod2.predict(test_x,test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.datasets as dsets\n",
    "import torchvision.transforms as transforms\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Dataset1 import catnoncat\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = catnoncat.train_x() \n",
    "test_x = catnoncat.test_img()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_y =catnoncat.train_y()\n",
    "test_y =catnoncat.test_y()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(209, 1) (209, 12288)\n"
     ]
    }
   ],
   "source": [
    "x_data = train_x.reshape(-1,train_x.shape[0]).T\n",
    "\n",
    "labels = train_y.reshape(train_y.shape[0], -1).T\n",
    "\n",
    "print(labels.shape,x_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50, 1) (50, 12288)\n"
     ]
    }
   ],
   "source": [
    "x_data1 = test_x.reshape(-1,test_x.shape[0]).T\n",
    "\n",
    "labels1 = test_y.reshape(test_y.shape[0], -1).T\n",
    "\n",
    "print(labels1.shape,x_data1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([100, 1])\n"
     ]
    }
   ],
   "source": [
    "train_data = []\n",
    "for i in range(len(x_data)):\n",
    "    train_data.append([x_data[i], labels[i]])\n",
    "\n",
    "trainloader = torch.utils.data.DataLoader(train_data, shuffle=True, batch_size=100)\n",
    "i1, l1 = next(iter(trainloader))\n",
    "print(l1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([50, 1])\n"
     ]
    }
   ],
   "source": [
    "test_data = []\n",
    "for i in range(len(x_data1)):\n",
    "    test_data.append([x_data1[i], labels1[i]])\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(test_data, shuffle=True, batch_size=100)\n",
    "i1, l1 = next(iter(test_loader))\n",
    "print(l1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model\n",
    "class LogisticRegression(nn.Module):\n",
    "    def __init__(self, input_size, num_classes):\n",
    "        super(LogisticRegression, self).__init__()\n",
    "        self.linear1 = nn.Linear(input_size, num_classes)\n",
    "        self.linear2 = nn.Linear(num_classes,1)\n",
    "        self.m = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.linear1(x)\n",
    "        out = F.relu(out)\n",
    "        out = self.linear2(out)\n",
    "        sout = self.m(out)\n",
    "        return sout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 12288\n",
    "num_classes = 1\n",
    "num_epochs = 7000\n",
    "batch_size = 100\n",
    "learning_rate = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the model on the 10000 test images: 1700 %\n"
     ]
    }
   ],
   "source": [
    "model = LogisticRegression(input_size, num_classes)\n",
    "\n",
    "# Loss and Optimizer\n",
    "# Softmax is internally computed.\n",
    "# Set parameters to be updated.\n",
    "criterion = nn.BCELoss() \n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)  \n",
    "\n",
    "# Training the Model\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (images, labels) in enumerate(trainloader):\n",
    "        images = Variable(images.float())\n",
    "        labels = Variable(labels.float())\n",
    "        # Forward + Backward + Optimize\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        #print(outputs.shape,labels.shape)\n",
    "\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if (i+1) % 100 == 0:\n",
    "            print ('Epoch: [%d/%d], Step: [%d/%d], Loss: %.4f' \n",
    "                   % (epoch+1, num_epochs, i+1, len(train_dataset)//batch_size, loss.data[0]))\n",
    "\n",
    "# Test the Model\n",
    "correct = 0\n",
    "total = 0\n",
    "for images, labels in test_loader:\n",
    "    images = Variable(images.float())\n",
    "    outputs = model(images)\n",
    "    _, predicted = torch.max(outputs.data, 1)\n",
    "    total += labels.float().size(0)\n",
    "    correct += (predicted == labels).sum()\n",
    "    \n",
    "print('Accuracy of the model on the 10000 test images: %d %%' % (100 * correct / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
